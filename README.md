# Agentic RAG System (FastAPI + LangGraph + Pinecone + OpenAI)

This project implements a **Retrieval-Augmented Generation (RAG)** system using:

- **LangGraph** â†’ Agentic orchestration (retriever â†’ LLM nodes flow)  
- **FastAPI** â†’ REST API service  
- **Pinecone** â†’ Vector database  
- **OpenAI** â†’ Embeddings + LLM (ChatGPT)  
- **PDF Support only** â†’ Document ingestion pipeline  

---

## Features

âœ… Upload a **PDF file** â†’ extract + chunk text â†’ embed â†’ store in Pinecone  
âœ… Query the system â†’ retrieve relevant chunks from Pinecone â†’ context-aware LLM answer  
âœ… Update or delete documents by `file_id`  
âœ… LangGraph workflow for modular, agentic orchestration  
âœ… FastAPI endpoints for easy integration  

---

## Project Structure

rag_app/
â”‚
â”œâ”€â”€ main.py # FastAPI entrypoint
â”‚
â”œâ”€â”€ api/ # API routes
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ routes_chat.py # /chat endpoint (LangGraph)
â”‚ â”œâ”€â”€ routes_files.py # /add_file, /delete_file, /update_file endpoints
â”‚
â”œâ”€â”€ core/ # Core configs
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ config.py # Load env vars (OpenAI + Pinecone)
â”‚
â”œâ”€â”€ services/ # Business logic
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ data_injestion_service.py # PDF text extraction + chunking
â”‚ â”œâ”€â”€ embeddings_service.py # OpenAI embeddings
â”‚ â”œâ”€â”€ llm_service.py # OpenAI llm for QA
â”‚ â”œâ”€â”€ vectordb_service.py # Pinecone insert, delete, query
â”‚
â”œâ”€â”€ utils/ # (optional utilities)
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ logger.py # Centralized logging
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # Documentation
â”œâ”€â”€ .env # Environment variables

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/JawadGondal/Agentic-RAG-Syste

```

### 2. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows

```

### 3. Install dependencies

```bash
pip install -r requirements.txt

```

### 4. Configure environment variables

Create a .env file in the root directory:

```bash
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
EMBEDDING_MODEL=text-embedding-3-small
PINECONE_API_KEY=...
PINECONE_CLOUD = aws
PINECONE_REGION = us-east-1
PINECONE_INDEX=rag-index
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

## â–¶ï¸ Running the Server

Start the FastAPI server:

```bash
uvicorn main:app --reload --port 8000
```

API docs available at:
ğŸ‘‰ Swagger UI: http://127.0.0.1:8000/docs
ğŸ‘‰ ReDoc: http://127.0.0.1:8000/redoc

## ğŸ“Œ API Endpoints

### 1. Add File (PDF ingestion)

```bash
POST /add_file
```

Description: Upload a PDF â†’ extract text â†’ chunk â†’ embed â†’ upsert into Pinecone.

Request:

Multipart file upload

Response:

```bash
{
  "file_id": "myfile-123",
  "message": "File successfully added."
}
```

### 2. Delete File

```bash
DELETE /delete_file/{file_id}
```

Description: Remove all vectors belonging to a file_id from Pinecone.

**Request**

```bash
file_id = myfile-123
```

Response:

```bash
{
  "message": "File myfile-123 deleted successfully."
}
```

### 3. Update File

```bash
PUT /update_file/{file_id}
```

Description: Replace an existing file with a new PDF (re-ingestion).

**Request**

```bash
file_id = myfile-123
```

Response:

```bash
{
  "message": "File myfile-123 updated successfully."
}
```

### 4. Chat (RAG Query)

```bash
POST /chat
```

Description: Retrieve relevant chunks from Pinecone + answer query via LangGraph â†’ OpenAI.

Request:

```bash
{
  "query": "What does the document say about data privacy?"
}
```


Response:

```bash
{
  "response": "The document states that data privacy is ensured through..."
}
```

ğŸ§  LangGraph Workflow

The ```bash/chat``` endpoint runs a LangGraph state machine:

User Query â†’ [Retriever Node] â†’ [LLM Node with Context] â†’ Answer


Retriever Node â†’ pulls relevant chunks from Pinecone

LLM Node â†’ generates answer with context

StateGraph â†’ ensures smooth data flow
